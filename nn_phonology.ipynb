{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn_phonology.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oserikov/nn_harmony_np/blob/master/nn_phonology.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRXwiDU17oCc",
        "colab_type": "code",
        "outputId": "4ac5765d-400e-454b-d4f8-eb27eed962eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WivJQeI4oZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYth0dk_miAO",
        "colab_type": "code",
        "outputId": "f334bb76-1fff-47f7-dbbb-f8264aa68ffe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "!git clone https://github.com/oserikov/nn_harmony_np.git\n",
        "%cd nn_harmony_np"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'nn_harmony_np'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 140 (delta 6), reused 1 (delta 0), pack-reused 125\u001b[K\n",
            "Receiving objects: 100% (140/140), 13.18 MiB | 14.76 MiB/s, done.\n",
            "Resolving deltas: 100% (46/46), done.\n",
            "/content/nn_harmony_np\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRh2_uBdob1x",
        "colab_type": "code",
        "outputId": "49c331f9-3061-4323-8b4c-353f64c7fbad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from nn_model import NNModel, ModelStateLogDTO\n",
        "from nn_model import NNModel, ModelStateLogDTO\n",
        "from experiment_datasets_creator import ExperimentCreator\n",
        "from phonology_tool import PhonologyTool\n",
        "from experiment_datasets_creator import ExperimentCreator\n",
        "from phonology_tool import PhonologyTool\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.externals.six import StringIO  \n",
        "from IPython.display import Image, display\n",
        "from sklearn.tree import export_graphviz\n",
        "import pydotplus\n",
        "import pickle"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4mN1BvGnYqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_fn = \"data/tur_apertium_words.txt\"\n",
        "\n",
        "\n",
        "train_dataset = []\n",
        "alphabet = set()\n",
        "with open(train_data_fn, 'r', encoding=\"utf-8\") as train_data_f:\n",
        "    for line in train_data_f:\n",
        "        train_dataset.append(line.strip())\n",
        "        [alphabet.add(c) for c in line.strip()]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0Ph7yriozh8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_mode = \"pretrained\"\n",
        "\n",
        "assert training_mode in [\"train\", \"pretrained\"]\n",
        "\n",
        "if training_mode == \"train\":\n",
        "    EPOCHS_NUM = 300\n",
        "\n",
        "\n",
        "    hidden_sizes = [2,3,4,5,6,7,8,9,10]\n",
        "    hidden_types = [\"sigmoid\", \"tanh\", \"relu\"]\n",
        "\n",
        "    model_filenames = []\n",
        "\n",
        "    for hidden_type in hidden_types:\n",
        "        for hidden_size in hidden_sizes:\n",
        "            model_filename = f\"model_size_{hidden_size}_activation_{hidden_type}.pkl\"\n",
        "\n",
        "            model = NNModel(alphabet, hidden_size, activation=hidden_type)\n",
        "\n",
        "            train_data = [(entry[:-1], entry[1:]) for entry in train_dataset]\n",
        "            for epoch_num, epoch_loss in model.train(train_data, EPOCHS_NUM):\n",
        "                print('\\t'.join([f\"hidden_size: {hidden_size}\", \n",
        "                                 f\"hidden_type:{hidden_type}\",\n",
        "                                 f\"epoch_num: {epoch_num}\", \n",
        "                                 f\"epoch_loss: {epoch_loss}\"]))\n",
        "\n",
        "            model.save(model_filename)\n",
        "            model_filenames.append(model_filename)\n",
        "\n",
        "            \n",
        "if training_mode == \"pretrained\":\n",
        "    !cp models/*.pkl ./\n",
        "    model_filenames = !ls *.pkl\n",
        "    model_filenames = [fn for fns in model_filenames for fn in fns.split()]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgGA1jVWp1vB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if training_mode == \"train\":\n",
        "    [files.download(model_filename) for model_filename in model_filenames]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YINOS5kc_mma",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "2a2116c1-f106-4df7-b657-255da91294fe"
      },
      "source": [
        "\n",
        "test_data_fn = \"data/tur_swadesh.txt\"\n",
        "phonology_features_filename = \"data/tur_phon_features.tsv\"\n",
        "\n",
        "datasets = []\n",
        "for model_filename in model_filenames:\n",
        "    model_filename_prefix = model_filename.rstrip(\".pkl\")\n",
        "\n",
        "    model = NNModel.load_model(model_filename)\n",
        "\n",
        "    test_dataset = []\n",
        "    with open(test_data_fn, 'r', encoding=\"utf-8\") as test_data_f:\n",
        "        for line in test_data_f:\n",
        "            if all(c in model.alphabet for c in line.strip()):\n",
        "                test_dataset.append(line.strip())\n",
        "\n",
        "    phonologyTool = PhonologyTool(phonology_features_filename)\n",
        "    experimentCreator = ExperimentCreator(model, test_dataset, phonologyTool)\n",
        "\n",
        "    # front_harmony_dataset\n",
        "    front_harmony_dataset_fn = model_filename_prefix+ \"_front_harmony_dataset.tsv\"\n",
        "    front_harmony_dataset = experimentCreator.make_dataset_pretty(experimentCreator.front_harmony_dataset())\n",
        "    experimentCreator.save_dataset_to_tsv(front_harmony_dataset, front_harmony_dataset_fn)\n",
        "    datasets.append(front_harmony_dataset_fn)\n",
        "    \n",
        "    # vov_vs_cons_dataset\n",
        "    vov_vs_cons_dataset_fn = model_filename_prefix+ \"_vov_vs_cons_dataset.tsv\"\n",
        "    vov_vs_cons_dataset = experimentCreator.make_dataset_pretty(experimentCreator.vov_vs_cons_dataset())\n",
        "    experimentCreator.save_dataset_to_tsv(vov_vs_cons_dataset, vov_vs_cons_dataset_fn)\n",
        "    datasets.append(vov_vs_cons_dataset_fn)\n",
        "    \n",
        "    # front_feature_dataset\n",
        "    front_feature_dataset_fn = model_filename_prefix+ \"_front_feature_dataset.tsv\"\n",
        "    front_feature_dataset = experimentCreator.make_dataset_pretty(experimentCreator.front_feature_dataset())\n",
        "    experimentCreator.save_dataset_to_tsv(front_feature_dataset, front_feature_dataset_fn)\n",
        "    datasets.append(front_feature_dataset_fn)\n",
        "    \n",
        "    # is_starting_consonant_cluster_dataset\n",
        "    is_starting_consonant_cluster_dataset_fn = model_filename_prefix+ \"_is_starting_consonant_cluster_dataset.tsv\"\n",
        "    is_starting_consonant_cluster_dataset = experimentCreator.make_dataset_pretty(experimentCreator.is_starting_consonant_cluster_dataset())\n",
        "    experimentCreator.save_dataset_to_tsv(is_starting_consonant_cluster_dataset, is_starting_consonant_cluster_dataset_fn)\n",
        "    datasets.append(is_starting_consonant_cluster_dataset_fn)\n",
        "    \n",
        "    # second_consonant_in_cluster_dataset\n",
        "    second_consonant_in_cluster_dataset_fn = model_filename_prefix+ \"_second_consonant_in_cluster_dataset.tsv\"\n",
        "    second_consonant_in_cluster_dataset = experimentCreator.make_dataset_pretty(experimentCreator.second_consonant_in_cluster_dataset())\n",
        "    experimentCreator.save_dataset_to_tsv(second_consonant_in_cluster_dataset, second_consonant_in_cluster_dataset_fn)\n",
        "    datasets.append(second_consonant_in_cluster_dataset_fn)\n",
        "    \n",
        "    # voiced_stop_consonant_dataset\n",
        "    voiced_stop_consonant_dataset_fn = model_filename_prefix+ \"_voiced_stop_consonant_dataset.tsv\"\n",
        "    voiced_stop_consonant_dataset = experimentCreator.make_dataset_pretty(experimentCreator.voiced_stop_consonant_dataset())\n",
        "    experimentCreator.save_dataset_to_tsv(voiced_stop_consonant_dataset, voiced_stop_consonant_dataset_fn)\n",
        "    datasets.append(voiced_stop_consonant_dataset_fn)\n",
        "    "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-c411f6cf1362>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0msecond_consonant_in_cluster_dataset_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_filename_prefix\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m\"_second_consonant_in_cluster_dataset.tsv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0msecond_consonant_in_cluster_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperimentCreator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset_pretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperimentCreator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msecond_consonant_in_cluster_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mexperimentCreator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_dataset_to_tsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecond_consonant_in_cluster_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_consonant_in_cluster_dataset_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecond_consonant_in_cluster_dataset_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/nn_harmony_np/experiment_datasets_creator.py\u001b[0m in \u001b[0;36msave_dataset_to_tsv\u001b[0;34m(dataset, dataset_fn)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mdictWriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_to_single_dicts_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mdictWriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteheader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mdictWriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_to_single_dicts_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;31m# print('\\t'.join(dataset_keys + [\"target\"]), file=dataset_f)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/csv.py\u001b[0m in \u001b[0;36mwriterows\u001b[0;34m(self, rowdicts)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwriterows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrowdicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dict_to_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrowdicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;31m# Guard Sniffer's type checking against builds that exclude complex()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/csv.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    150\u001b[0m                 raise ValueError(\"dict contains fields not in fieldnames: \"\n\u001b[1;32m    151\u001b[0m                                  + \", \".join([repr(x) for x in wrong_fields]))\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrowdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfieldnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrowdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3UyEB8NdABr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tree_to_pseudo(tree, features_names):\n",
        "    left = tree.tree_.children_left\n",
        "    right = tree.tree_.children_right\n",
        "    threshold = tree.tree_.threshold\n",
        "    features = [features_names[i] for i in tree.tree_.feature]\n",
        "    value = tree.tree_.value\n",
        "\n",
        "    nodes = []\n",
        "\n",
        "    def recurse(left, right, threshold, features, node, depth=0):\n",
        "        indent = \"  \" * depth\n",
        "        if (threshold[node] != -2):\n",
        "            # print (indent,\"if ( \" + features[node] + \" <= \" + str(threshold[node]) + \" ) {\")\n",
        "            nodes.append({\"id\":len(nodes),\n",
        "                          \"feature\": features[node], \n",
        "                          \"threshold\": threshold[node]})\n",
        "            if left[node] != -1:\n",
        "                recurse (left, right, threshold, features, left[node], depth+1)\n",
        "                # print (indent,\"} else {\")\n",
        "            if right[node] != -1:\n",
        "                recurse (left, right, threshold, features, right[node], depth+1)\n",
        "                # print (indent,\"}\")\n",
        "        else:\n",
        "            pass\n",
        "            # print (indent,\"return \" + str(value[node]))\n",
        "    \n",
        "    recurse(left, right, threshold, features, 0)\n",
        "    return nodes\n",
        "\n",
        "# tree_to_pseudo(d_tree, feature_colnames)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0t-TpkLc2Ig",
        "colab_type": "code",
        "outputId": "adf55bd3-fe42-44d3-9a4c-60c0f922e23a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        }
      },
      "source": [
        "for task_fn in datasets:\n",
        "    \n",
        "    task_df = pd.read_csv(task_fn, delimiter='\\t', encoding=\"utf-8\")\n",
        "\n",
        "    feature_colnames = [colname \n",
        "                        for colname in task_df.columns[2:-1] \n",
        "                        if \"hidden_outs\" in colname]\n",
        "#                         or \"output_ins\" in colname]\n",
        "    target_colname = task_df.columns[-1]\n",
        "    task_df_g = task_df.groupby(target_colname)\n",
        "    task_df = task_df_g.apply(lambda x: x.sample(task_df_g.size().min())).reset_index(drop=True)\n",
        "    \n",
        "    for column in task_df.columns:\n",
        "        if pd.api.types.is_numeric_dtype(task_df[column])\\\n",
        "        and not pd.api.types.is_bool_dtype(task_df[column]):\n",
        "            task_df[column+\"_mean\"] = task_df[column].mean()\n",
        "\n",
        "    task_df.to_csv(task_fn.replace('.tsv', '_balanced.tsv'), \n",
        "                   sep='\\t', encoding=\"utf-8\")\n",
        "    y = task_df[target_colname]\n",
        "    X = task_df[feature_colnames]\n",
        "\n",
        "    dt = DecisionTreeClassifier(max_depth=2)\n",
        "    dt.fit(X, y)\n",
        "    accuracy = dt.score(X, y)\n",
        "    prediction = dt.predict(X)\n",
        "    dt_nodes = tree_to_pseudo(dt, feature_colnames)\n",
        "    for dt_node in dt_nodes:\n",
        "        dt_node_id = dt_node[\"id\"]\n",
        "        dt_node_feature = dt_node[\"feature\"]\n",
        "        dt_node_treshold = dt_node[\"threshold\"]\n",
        "        dt_node_feature_colname = f\"node_{dt_node_id}_feature\"\n",
        "        dt_node_treshold_colname = f\"node_{dt_node_id}_threshold\"\n",
        "        dt_node_condition_colname = f\"node_{dt_node_id}_condition\"\n",
        "        \n",
        "        task_df[dt_node_feature_colname] = dt_node_feature\n",
        "        task_df[dt_node_treshold_colname] = dt_node_treshold\n",
        "        dt_node_condition = task_df[dt_node_feature] <= dt_node_treshold\n",
        "        task_df[dt_node_condition_colname] = dt_node_condition\n",
        "\n",
        "    accuracy_colname = \"accuracy\"\n",
        "    task_df[accuracy_colname] = accuracy\n",
        "\n",
        "    prediction_colname = \"prediction\"\n",
        "    task_df[prediction_colname] = prediction\n",
        "    \n",
        "    task_df.to_csv(task_fn, sep='\\t')\n",
        "\n",
        "\n",
        "\n",
        "    model_size = int([e for e in task_fn.split('_') if e.isnumeric()][0])\n",
        "   \n",
        "    # or \"voiced_stop\" in task_fn or \"vov_vs_cons\" in task_fn \\\n",
        "  \n",
        "    # if (accuracy < 0.8 or model_size > 8) \\\n",
        "    # or \"front_feature\" in task_fn or \"front_harmony\" in task_fn:# or accuracy < 0.9:\n",
        "    #     continue\n",
        "\n",
        "    tree_dot_fn = task_fn.replace(\".tsv\", \".dot\")\n",
        "    tree_pkl_fn = task_fn.replace(\".tsv\", \".pkl\")\n",
        "    with open(tree_pkl_fn, 'wb') as f: pickle.dump(dt, f)\n",
        "    # dot_data = StringIO()\n",
        "    export_graphviz(dt, out_file=tree_dot_fn, special_characters=True, feature_names=X.columns, \n",
        "                    class_names=True, proportion=True, impurity=False)\n",
        "    with open(tree_dot_fn, 'r', encoding=\"utf-8\") as tree_f:\n",
        "        dot_data = tree_f.read()\n",
        "    graph = pydotplus.graph_from_dot_data(dot_data)#dot_data.getvalue())  \n",
        "    display(Image(graph.create_png()))\n",
        "    print(task_fn)\n",
        "    print(\"accuracy: \", accuracy)\n",
        "\n",
        "    print(\"======================\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "\n",
        "    "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK8AAAA9CAYAAADbPDk9AAAABmJLR0QA/wD/AP+gvaeTAAANqElE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\nmNvDDNoLI14GrYURL4PW8j8AUkXPHmgDFQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "model_size_10_activation_relu_front_harmony_dataset.tsv\n",
            "accuracy:  1.0\n",
            "======================\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-f52ba2ef18ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    878\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_classification\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    167\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[1;32m    168\u001b[0m                       'multilabel-indicator', 'multilabel-sequences']:\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unknown label type: 'continuous'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I579Zz446OZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir analysis_data && cp *.tsv *.dot *.pkl analysis_data && zip -r analysis_data.zip analysis_data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-ay0kbV7359",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv analysis_data.zip /content/drive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}